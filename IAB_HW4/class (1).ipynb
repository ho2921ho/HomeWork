{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session #1 \n",
    "## Dictionary-based SA using SentiStrength dictionary\n",
    "\n",
    "첫 번째 세션에서는 SentiStrength에 탑재된 사전을 활용하여 간단한 사전 기반 (dictionary-based) 영어 텍스트 감성분석 프로그램을 작성해 봅니다.  \n",
    "실습 수업은 프로그램의 주요 흐름을 설명하면서, TODO 처리된 핵심적인 부분의 코드를 직접 작성해보는 순서로 진행될 것입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사전 데이터와 테스트에 사용할 텍스트 파일이 있는 폴더를 지정합니다.  \n",
    "여기서는 사전 데이터가 'SentiStrength_Data' 폴더에, 텍스트 파일이 '6humanCodedDataSets' 폴더에 있는 것을 전제로 하였습니다.  \n",
    "만일 사전 데이터와 텍스트 파일이 다른 폴더에 있을 경우 이를 변경하셔야 합니다.  \n",
    "(코드 .ipynb 파일과 같은 폴더에 있을 시에는 './'로 설정하여 주십시오)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_dir = 'SentiStrength_Data/'\n",
    "data_dir = '6humanCodedDataSets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str =\"???????????!!!!!!!!222\"\n",
    "str1 = re.sub(r\"[!]\", \"\",str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'???????????222'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Tokenizing\n",
    "문장을 먼저 단어 단위로 분할하는 작업을 수행합니다. 영어 텍스트인 만큼 기본적으로 띄어쓰기 단위로 분할을 하되, 문장 부호는 삭제하고, 대문자는 소문자로 치환하도록 합니다.  \n",
    "여기서는 정규 표현식과 re.sub() 함수를 활용하여 문장 부호 (? ! \" . , ;)를 제거하는 부분을 직접 작성해봅시다.  \n",
    "작성 후 함수를 직접 실행하여 의도대로 구현되었는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Parse a given sentence to a list\n",
    "Input: sentence string\n",
    "Output: List of words\n",
    "'''\n",
    "def parse_input(str):\n",
    "    import re   #import regular expressions module\n",
    "    # TODO : remove punctuations using regular expression and re.sub() function\n",
    "    # for example, using str = re.sub(r'[abc]','',str) to remove a character a, b or c from string str.\n",
    "    \n",
    "    str = str.lower()\n",
    "    #make all characters lowercase\n",
    "    str = re.sub(r'[!?\".]', \"\",str)\n",
    "    words = str.split(' ') #split words in string on space and save them in a list.\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str ='Why the deafening silence?  It shows the duplicity of \"the allies\".'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str= str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str = re.sub(r'[!?\".]', \"\",str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['why',\n",
       " 'the',\n",
       " 'deafening',\n",
       " 'silence',\n",
       " '',\n",
       " 'it',\n",
       " 'shows',\n",
       " 'the',\n",
       " 'duplicity',\n",
       " 'of',\n",
       " 'the',\n",
       " 'allies']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict = \"accus*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['why',\n",
       " 'the',\n",
       " 'deafening',\n",
       " 'silence',\n",
       " '',\n",
       " 'it',\n",
       " 'shows',\n",
       " 'the',\n",
       " 'duplicity',\n",
       " 'of',\n",
       " 'the',\n",
       " 'allies']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_input('Why the deafening silence?  It shows the duplicity of \"the allies\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = \"abandon*\t-2\tliwc uness specified otherwise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A=A.split(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abandon*', '-2', 'liwc uness specified otherwise']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Parsing dictionaries\n",
    "여기에서는 감성분석에 사용할 각 사전들을 읽어옵니다.\n",
    "사전은 크게 2가지 형태가 사용되는데, 하나는 각 단어마다 고유한 weight가 부여된 사전이며, 나머지 하는 부정어 (negating word)가 저장된 사전인데 부정어의 경우 별도의 weight를 가지지 않으므로 부정어 사전의 모든 weight는 0이 됩니다.\n",
    "그래서 각각에 대해 별도의 사전 생성 함수를 작성할 것입니다. Python의 dictionary 자료형을 이용합니다.\n",
    "\n",
    "1) weight가 저장된 dictionary를 불러오는 함수입니다.  \n",
    "해당 사전들은 매 줄이 \"단어 (탭) weight (탭) 주석\" 형식으로 저장되어 있습니다.  \n",
    "이를 읽어서 저장하는 코드를 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Parse boosting weights\n",
    "Input: file name\n",
    "Output: dictionary of weights\n",
    "'''\n",
    "def parse_weight(fname):\n",
    "    w = {}   #dictionary\n",
    "    with open(fname, 'r') as f :\n",
    "        for line in f: #for each line in the file,\n",
    "            # TODO : add a dictionary entry from each line\n",
    "            \n",
    "            rule = line.split(\"\\t\")\n",
    "            w[rule[0]] = int(rule[1])\n",
    "            # You first split the line into a word, a weight and a comment, then add the entry to the dictionary\n",
    "            # Hint : you may use the split() function\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) negating word에 대한 dictionary를 불러오는 함수입니다.  \n",
    "negating word 사전은 매 줄 마다 단어만 저장되어 있습니다.  \n",
    "따라서 weight는 모두 0으로 저장합니다. 이를 감안하여 함수를 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Parse negations\n",
    "Input: file name\n",
    "Output: dictionary of weights (all 0)\n",
    "'''\n",
    "def parse_negate(fname):\n",
    "    w = {}   #dictionary\n",
    "    with open(fname, 'r') as f :\n",
    "        for line in f: #for each line in the file\n",
    "            w[line[:-2]] = 0\n",
    "            # TODO : add a dictionary entry from each line\n",
    "            # Unlike other dictionaries, the negating word dictionary only has a word in each line\n",
    "            # An weight for each line should be assigned as '0'.\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./SentiStrength_Data/NegatingWordList.txt\", \"r\") as f:\n",
    "    w= {}\n",
    "    for line in f:\n",
    "        w[line[:-2]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"aren't\": 0,\n",
       " 'arent': 0,\n",
       " \"can't\": 0,\n",
       " 'cannot': 0,\n",
       " 'cant': 0,\n",
       " \"couldn't\": 0,\n",
       " 'couldnt': 0,\n",
       " \"don't\": 0,\n",
       " 'dont': 0,\n",
       " \"isn't\": 0,\n",
       " 'isnt': 0,\n",
       " 'never': 0,\n",
       " 'not': 0,\n",
       " \"won't\": 0,\n",
       " 'wont': 0,\n",
       " 'would': 0,\n",
       " \"wouldn't\": 0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 사전 데이터를 통해 구현된 함수를 확인합니다.  \n",
    "실행 결과는 2, 0 으로 나와야 합니다. (각각 extremely와 never에 대한 weight)  \n",
    "필요한 경우 다른 단어에 대해 사전을 확인해 보실 수도 있습니다. (예 : dict_boost['very'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str =\"aaaaaaaaaaaab\"\n",
    "strminus1 = str[2:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "substr = \"aggressive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "substr = \"aggress*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2, 0\n"
     ]
    }
   ],
   "source": [
    "dict_boost = parse_weight(dict_dir + 'BoosterWordList.txt')\n",
    "dict_negate = parse_negate(dict_dir + 'NegatingWordList.txt')\n",
    "print('%d, %d'%(dict_boost['extremely'],dict_negate['never']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'absolutely': 1,\n",
       " 'complete': 1,\n",
       " 'completely': 1,\n",
       " 'could': -1,\n",
       " 'definitely': 1,\n",
       " 'did': -1,\n",
       " 'especially': 1,\n",
       " 'extremely': 2,\n",
       " 'fuckin': 2,\n",
       " 'fucking': 2,\n",
       " 'hugely': 2,\n",
       " 'incredibly': 2,\n",
       " 'just': -1,\n",
       " 'may': -1,\n",
       " 'might': -1,\n",
       " 'ought': -1,\n",
       " 'overwhelmingly': 2,\n",
       " 'really': 1,\n",
       " 'should': -1,\n",
       " 'slightly': -1,\n",
       " 'so': 0,\n",
       " 'some': -1,\n",
       " 'sometimes': -1,\n",
       " 'sum': -1,\n",
       " 'total': 1,\n",
       " 'totally': 1,\n",
       " 'very': 1,\n",
       " 'would': -1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Weighting\n",
    "구축한 dictionary를 이용해 주어진 문장의 positive/negative strength (weight)를 찾아냅니다.  \n",
    "즉 Parsing한 단어들의 weight를 주어진 모델의 dictionary에서 찾는 과정입니다.  \n",
    "사전에는 'ador*' 와 같이 어두에 대한 정의가 존재하므로 전체 단어가 사전에 없어도 어두를 통해 weight를 부여하는 경우도 고려해야 합니다.  \n",
    "이를 위해 단어가 사전에 존재하지 않는 경우 끝에서부터 1 글자씩 잘라가면서 어두를 찾는 루틴도 구현합니다.  \n",
    "그렇게 해도 사전에 존재하지 않는 단어는 0으로 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Give weights on words\n",
    "Input: list of words, dictionary of weights\n",
    "Output: list of (word, weight) pairs\n",
    "'''\n",
    "def weight_default(list_words, dic_weights):\n",
    "    l = []  #list\n",
    "    for word in list_words:  #for each word in the given list,\n",
    "        if word in dic_weights:          \n",
    "            l.append((word, dic_weights[word]))  #add word and its weight if the word is in the dictionary\n",
    "        else:       #if the word is not in the dictionary, check if the words\n",
    "            substr = word  #first, we define substr as the word\n",
    "            while substr != '':  #while substr is not NULL,\n",
    "                pat = substr + '*'   #check if substr* is in the dictionary\n",
    "                if pat in dic_weights:   \n",
    "                    l.append((word, dic_weights[pat]))\n",
    "                    break   #break means we stop the loop and move on to the next word.\n",
    "                else:\n",
    "                    substr = substr[:-1]\n",
    "                    # TODO : trim the last character of substr before the loop iterates\n",
    "                    \n",
    "            else: # no matching word found in the dictionary\n",
    "                l.append((word, 0))  #if a matching word is not exist on the dictionary, set its weight as '0'\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Evaluation\n",
    "본 실습에서는 문장이 부정적/긍정적 감성인지 판단할 때 문장의 각 단어가 가지는 positive strength의 최대값과 negative strength의 최소값을 비교하여, 절대값의 대소에 따라서 positive / neutral / negative를 평가합니다.  \n",
    "가령 positive의 최대값이 4이고, negative의 최소값이 -3이면 그 문장을 positive sentiment를 가졌다고 평가합니다.  \n",
    "반면 positive의 최대값이 3이고 negative의 최소값이 -4이면 negative, 두 절대값이 서로 같으면 neutral로 평가해야 합니다.  \n",
    "아래 extract_max() 함수를 작성하여 (word, weight)의 list가 들어왔을 때 (positive의 최대값, negative의 최소값)을 반환하도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Extract maximum weights for given (word, weight) pairs\n",
    "Input: list of (word, weight)\n",
    "Output: (positive, negative)\n",
    "'''\n",
    "def extract_max(list_pairs):\n",
    "    pos_max = 1  #set default of the maximum positive strength as '1'\n",
    "    neg_min = -1  #set default of the maximum negative strength as '-1'\n",
    "\n",
    "    for p in list_pairs:\n",
    "        ## TODO : Iterate over the list of (word, weight) pairs, and extract the max of positive weights and min of negative weights.\n",
    "        if (p[1]>pos_max):\n",
    "            pos_max = p[1]\n",
    "        elif (p[1]<neg_min):\n",
    "            neg_min = p[1]\n",
    "\n",
    "    return (pos_max, neg_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Summing up\n",
    "앞서 구현한 함수들을 종합해 문장을 입력받아 sentiment strength를 출력 및 감성 평가를 수행하는 루틴을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i : 0)\n",
      "(am : 0)\n",
      "(totally : 0)\n",
      "(happy : 2)\n",
      "(and : 0)\n",
      "(hot : 0)\n",
      "Max. of positive strength : 2, Min. of negative strength : -1\n",
      "Positive Sentence\n"
     ]
    }
   ],
   "source": [
    "sentence = 'i am totally happy and hot'\n",
    "words = parse_input(sentence)\n",
    "\n",
    "# TODO : parse the emotion dictionary by parsing 'EmotionLookupTable.txt' on the dictionary folder.\n",
    "dict_emotion = parse_weight(\"./SentiStrength_Data/EmotionLookupTable.txt\")\n",
    "\n",
    "\n",
    "# TODO : perform a default weighting using the emotion dictionary parsed above.\n",
    "words_and_weights = weight_default(words, dict_emotion)\n",
    "\n",
    "for pair in words_and_weights :\n",
    "    print('(%s : %d)'%(pair[0],pair[1]))\n",
    "\n",
    "pos_max, neg_min = extract_max(words_and_weights)\n",
    "print('Max. of positive strength : %d, Min. of negative strength : %d'%(pos_max,neg_min))\n",
    "\n",
    "if abs(pos_max) > abs(neg_min):\n",
    "    print(\"Positive Sentence\")\n",
    "elif abs(pos_max) < abs(neg_min):\n",
    "    print (\"Negative sentence\")\n",
    "else:\n",
    "    print(\"Neutral Sentence\")\n",
    "\n",
    "# TODO : make a final decision by comparing the absolute value of pos_max and neg_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 0), ('am', 0), ('totally', 0), ('happy', 2), ('and', 0), ('hot', 0)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_and_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am really goodi am good really'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'i am really good' 'i am good really'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Adding rules\n",
    "사전을 추가하여 감성분석이 좀 더 정밀하게 이루어질 수 있게 weight를 부여하는 룰을 추가하고자 합니다.\n",
    "\n",
    "#### 6-1. Boosting words\n",
    "뒤이은 단어의 의미를 강조하는 단어들로, 뒷 단어의 weighting을 강화하거나 약화시키는 룰이 미리 정의되어 있습니다.  \n",
    "weight_boost() 함수를 통해 이를 반영하고자 합니다. 앞서 weight_default()를 통해 나온 (word, weight)의 list를 입력받아 boosting 된 (word, weight)의 list를 반환해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Give boosting to another word\n",
    "Input: list of (word, weight)\n",
    "Output: list of (word, weight)\n",
    "'''\n",
    "def weight_boost(list_pairs, dic_boost):\n",
    "    l = []\n",
    "    boost = 0    #set default of the boost as '0'\n",
    "    for p in list_pairs:  #for each (word, weight) in the given list,\n",
    "        w = p[1]   # save its weight\n",
    "        if boost != 0:  \n",
    "            if w>0:\n",
    "                w += boost\n",
    "            elif w<0:\n",
    "                w -= boost\n",
    "            boost = 0    \n",
    "            l.append((p[0], w))\n",
    "          \n",
    "            #if the previous word is the boosting word,\n",
    "            # TODO : strengthen the weight of current word and add to the words list.\n",
    "            # Note that positive weights should be increased (+boost)\n",
    "            # and negative one should be decreased (-boost).\n",
    "            # reset the boost\n",
    "            \n",
    "        else:\n",
    "            l.append(p)  #if the previous word is not the boosting word, save the original (word, weight) into the output list.\n",
    "        if p[0] in dic_boost:  #check if the word is boosting word\n",
    "            boost = dic_boost[p[0]] #save the boosting weight\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 boost dictionary를 읽어들였으므로, 이를 사용해 구현된 weight_boost가 잘 동작하는 지 확인합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i : 0)\n",
      "(just : 0)\n",
      "(want : 0)\n",
      "(to : 0)\n",
      "(know : 0)\n",
      "(where : 0)\n",
      "(you : 0)\n",
      "(got : 0)\n",
      "(that : 0)\n",
      "(totally : 0)\n",
      "(awesome : 4)\n",
      "(wallpaper : 0)\n"
     ]
    }
   ],
   "source": [
    "sentence = 'i just want to know where you got that totally awesome wallpaper.'\n",
    "words = parse_input(sentence)\n",
    "words_and_weights1 = weight_default(words, dict_emotion)\n",
    "# You can see that how weight_boost() method works by commenting out following line\n",
    "words_and_weights = weight_boost(words_and_weights1, dict_boost)\n",
    "for pair in words_and_weights :\n",
    "    print('(%s : %d)'%(pair[0],pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 0),\n",
       " ('just', 0),\n",
       " ('want', 0),\n",
       " ('to', 0),\n",
       " ('know', 0),\n",
       " ('where', 0),\n",
       " ('you', 0),\n",
       " ('got', 0),\n",
       " ('that', 0),\n",
       " ('totally', 0),\n",
       " ('awesome', 3),\n",
       " ('wallpaper', 0)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_and_weights1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-2. Negating words\n",
    "뒤이은 단어를 부정하는 의미를 추가하는 단어들로, 뒤이인 sentiment word의 strength를 반전시킵니다.  \n",
    "여기서는 뒤이은 단어가 positive strength를 가질 경우 -0.5를 곱하고, negative인 경우 0으로 만들도록 함수를 작성하시겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Negate the weight for a word\n",
    "Input: list of (word, weight)\n",
    "Output: list of (word, weight)\n",
    "'''\n",
    "\n",
    "def weight_negate(list_pairs, dic_negate):\n",
    "    l = []\n",
    "    negate = 0   #set default of the negation as '0'\n",
    "    for p in list_pairs:  #for each (word, weight) in the given list,\n",
    "        w = p[1]  #save its weight\n",
    "        if negate != 0: #if the previous word is the negating word, apply negating rules\n",
    "            if w>0:\n",
    "                w *= -1\n",
    "            elif w<0:\n",
    "                w = 0\n",
    "            # TODO : multiply -1 for positivie weight, set to 0 for negative weight\n",
    "    \n",
    "            negate = 0   #reset the negation as its default value\n",
    "            l.append((p[0], int(w)))  #save the word and its changed weight\n",
    "        else:\n",
    "            l.append(p)  #if the previous word is not the negating word, save the original (word, weight) into the output list.\n",
    "        if p[0] in dic_negate: #check if the word is negating word\n",
    "            negate = 1  #if it is, set the negation as '1' just to indicate this case(you can choose other integers except '0'.)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"not bad\" :긍정적이냐? 중립적인것!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마찬가지로 weight_negate가 잘 작동하는 지 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i : 0)\n",
      "(don't : 0)\n",
      "(like : -2)\n",
      "(to : 0)\n",
      "(say : 0)\n",
      "(about : 0)\n",
      "(such : 0)\n",
      "(a : 0)\n",
      "(boring : -2)\n",
      "(thing : 0)\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I don\\'t like to say about such a boring thing.'\n",
    "words = parse_input(sentence)\n",
    "words_and_weights = weight_default(words, dict_emotion)\n",
    "# You can see that how weight_negate() method works by commenting out following line\n",
    "words_and_weights_negate = weight_negate(words_and_weights, dict_negate)\n",
    "for pair in words_and_weights_negate :\n",
    "    print('(%s : %d)'%(pair[0],pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 0),\n",
       " (\"don't\", 0),\n",
       " ('hate', -4),\n",
       " ('to', 0),\n",
       " ('say', 0),\n",
       " ('about', 0),\n",
       " ('such', 0),\n",
       " ('a', 0),\n",
       " ('boring', -2),\n",
       " ('thing', 0)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_and_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Wrapping up\n",
    "지금까지 구현한 weight 부여/변형 규칙으로, 텍스트 파일이 주어졌을 때 감성분석을 수행하는 루틴을 작성해 봅니다.  \n",
    "프로그램 작성을 용이하게 하기 위해 감성분석 값을 반환하는 함수를 먼저 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Our target\n",
    "Input: Some sentence\n",
    "Output: Positive, Negative strength\n",
    "'''\n",
    "def senti(str):    #this function is to apply above functions to the given string(str).\n",
    "    \n",
    "    # Parsing Rules\n",
    "    w_emotion = parse_weight(dict_dir + 'EmotionLookupTable.txt')  \n",
    "    w_boost = parse_weight(dict_dir + 'BoosterWordList.txt')\n",
    "    w_negate = parse_negate(dict_dir + 'NegatingWordList.txt')\n",
    "    \n",
    "    # Parse Input\n",
    "    words = parse_input(str)\n",
    "    \n",
    "    # Weighting\n",
    "    default = weight_default(words, w_emotion)\n",
    "    boosted = weight_boost(default, w_boost)\n",
    "    negated = weight_negate(boosted, w_negate)\n",
    "    \n",
    "    return extract_max(negated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, -2)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti('I don\\'t like to say about such a boring thing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로는 지정된 파일에서 문장들을 읽어들여서 감성 분석을 수행하고, 파일에 이미 기록된 결과값 (사람이 직접 평가한 점수)와 비교하는 프로그램을 작성합니다.  \n",
    "텍스트 파일을 읽을 때 주의사항은 첫 번째 라인은 건너뛰고 감성 분석을 수행하셔야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'2', u'2')\n",
      "(2, -1)\n",
      "(u'2', u'3')\n",
      "(2, -2)\n",
      "(u'3', u'1')\n",
      "(2, -1)\n",
      "(u'2', u'2')\n",
      "(2, -1)\n",
      "(u'3', u'1')\n",
      "(3, -2)\n",
      "(u'3', u'2')\n",
      "(2, -1)\n",
      "(u'1', u'1')\n",
      "(1, -1)\n",
      "(u'4', u'1')\n",
      "(3, -1)\n",
      "(u'3', u'1')\n",
      "(2, -2)\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "textFileName = data_dir + '1041MySpace.txt'\n",
    "scores = []\n",
    "\n",
    "with codecs.open(textFileName, 'r', encoding=\"utf-8\") as f :\n",
    "    lines = f.readlines()\n",
    "    for l in lines[1:10]:\n",
    "        l = l.split(\"\\t\")\n",
    "        print((l[0],l[1]))\n",
    "        print(senti(l[2]))\n",
    "        scores.append((int(l[0]),int(l[1]),senti(l[2])[0], -senti(l[2])[1]))\n",
    "        #l[0]:pos score  l[1]: neg score  l[2]: sentence\n",
    "        # senti(l[2]) ->(pos_score, negscore)\n",
    "        #print(l[0],l[1]) \"vs\" (senti(l[2]))\n",
    "        # \n",
    "    # TODO : for each line on the text file, run the senti() function \n",
    "    # and print the human evaluated and calculated scores.\n",
    "    # Note that the 1st line of each text file contains column names for each field, so this should be ignored.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 2, 2, 1),\n",
       " (2, 3, 2, 2),\n",
       " (3, 1, 2, 1),\n",
       " (2, 2, 2, 1),\n",
       " (3, 1, 3, 2),\n",
       " (3, 2, 2, 1),\n",
       " (1, 1, 1, 1),\n",
       " (4, 1, 3, 1),\n",
       " (3, 1, 2, 2)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2\\t2\\they witch wat cha been up too\\r\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 파일 전체에 대한 결과를 프린트할 경우 너무 많아질 수 있기 때문에 숫자를 입력받아 일정 문장만큼만 출력하도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Human evaluated pos / neg), (our pos / neg)\n",
      "(2/2),(2/1)\n",
      "(2/3),(2/2)\n",
      "(3/1),(2/1)\n",
      "(2/2),(2/1)\n",
      "(3/1),(3/2)\n",
      "(3/2),(2/1)\n",
      "(1/1),(1/1)\n",
      "(4/1),(3/1)\n",
      "(3/1),(2/2)\n"
     ]
    }
   ],
   "source": [
    "num_to_print = 9\n",
    "\n",
    "print('(Human evaluated pos / neg), (our pos / neg)')\n",
    "for i in range(num_to_print) :\n",
    "    print('(%d/%d),(%d/%d)'%(scores[i][0],scores[i][1],scores[i][2],scores[i][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
